import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, GRU
import matplotlib.pyplot as plt

# 
def load_txt_data(filename):
    data = []
    with open(filename, "r") as file:
        for line in file:
            values = list(map(float, line.strip().split(",")))  # ì‰¼í‘œ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬ í›„ float ë³€í™˜
            data.append(values)
    
    return np.array(data)

# 
filename = "M2501/D0125/yaw_data.txt"  # ì‚¬ìš©í•  TXT íŒŒì¼ ì´ë¦„
data = load_txt_data(filename)

X = data[:, :-1]  # ë§ˆì§€ë§‰ ì—´ì„ ì œì™¸í•œ 5ê°œì˜ feature
y = data[:, -1]   # ë§ˆì§€ë§‰ ì—´ì´ label

# 4ï¸âƒ£ RNN ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (batch_size, time_steps, features)
time_steps = X.shape[1]  # Feature ê°œìˆ˜ë¥¼ Time Stepìœ¼ë¡œ ì‚¬ìš©
features = 1             # ë‹¨ì¼ featureì”© ì…ë ¥

X_reshaped = X.reshape((X.shape[0], time_steps, features))
y_reshaped = y.reshape((-1, 1))  # Labelë„ (batch_size, 1) í˜•íƒœ

# 5ï¸âƒ£ Train, Validation, Test ë°ì´í„° ë‚˜ëˆ„ê¸°
X_train, X_temp, y_train, y_temp = train_test_split(X_reshaped, y_reshaped, test_size=0.2, random_state=42)  # 80% Train, 20% Temp
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 10% Validation, 10% Test

# ë°ì´í„° í˜•íƒœ ì¶œë ¥
print("X_train Shape:", X_train.shape)  # (Train ìƒ˜í”Œ ê°œìˆ˜, Time Steps, Features)
print("X_val Shape:", X_val.shape)      # (Validation ìƒ˜í”Œ ê°œìˆ˜, Time Steps, Features)
print("X_test Shape:", X_test.shape)    # (Test ìƒ˜í”Œ ê°œìˆ˜, Time Steps, Features)

# 6ï¸âƒ£ LSTM ëª¨ë¸ ìƒì„±
model_1 = Sequential([
    SimpleRNN(10, activation='tanh', input_shape=(time_steps, features)),  # SimpleRNN Layer
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜ìš© ì¶œë ¥ì¸µ
])

# 7ï¸âƒ£ ëª¨ë¸ ì»´íŒŒì¼
model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_1.summary()

# 8ï¸âƒ£ ëª¨ë¸ í•™ìŠµ (Validation Set í¬í•¨)
history_1 = model_1.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val))

# 9ï¸âƒ£ ëª¨ë¸ í‰ê°€ (Test Set)
test_loss_1, test_acc_1 = model_1.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc_1:.4f}")

# ğŸ”Ÿ ì˜ˆì¸¡ ì‹¤í–‰
predictions_1 = model_1.predict(X_test)
print("Predictions:", predictions_1.flatten())

model_2 = Sequential([
    LSTM(10, activation='tanh', input_shape=(time_steps, features)),  # SimpleRNN Layer
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜ìš© ì¶œë ¥ì¸µ
])

# 7ï¸âƒ£ ëª¨ë¸ ì»´íŒŒì¼
model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_2.summary()

# 8ï¸âƒ£ ëª¨ë¸ í•™ìŠµ (Validation Set í¬í•¨)
history_2 = model_2.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val))

# 9ï¸âƒ£ ëª¨ë¸ í‰ê°€ (Test Set)
test_loss_2, test_acc_2 = model_2.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc_2:.4f}")

# ğŸ”Ÿ ì˜ˆì¸¡ ì‹¤í–‰
predictions_2 = model_2.predict(X_test)
print("Predictions:", predictions_2.flatten())

model_3 = Sequential([
    GRU(10, activation='tanh', input_shape=(time_steps, features)),  # SimpleRNN Layer
    Dense(1, activation='sigmoid')  # ì´ì§„ ë¶„ë¥˜ìš© ì¶œë ¥ì¸µ
])

# 7ï¸âƒ£ ëª¨ë¸ ì»´íŒŒì¼
model_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model_3.summary()

# 8ï¸âƒ£ ëª¨ë¸ í•™ìŠµ (Validation Set í¬í•¨)
history_3 = model_3.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_val, y_val))

# 9ï¸âƒ£ ëª¨ë¸ í‰ê°€ (Test Set)
test_loss_3, test_acc_3 = model_3.evaluate(X_test, y_test)
print(f"Test Accuracy: {test_acc_2:.4f}")

# ğŸ”Ÿ ì˜ˆì¸¡ ì‹¤í–‰
predictions_3 = model_3.predict(X_test)
print("Predictions:", predictions_3.flatten())


def plot_training_history(history):
    plt.figure(figsize=(12, 4))

    # Loss ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Loss Over Epochs')
    plt.legend()

    # Accuracy ê·¸ë˜í”„
    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracy Over Epochs')
    plt.legend()

    plt.show()

# í•™ìŠµ ê·¸ë˜í”„ í˜¸ì¶œ
def plot_comparison(history_1, history_2, history_3):
    plt.figure(figsize=(12, 6))

    # Loss ë¹„êµ
    plt.subplot(1, 2, 1)
    plt.plot(history_1.history['loss'], label='SimpleRNN - Train Loss')
    plt.plot(history_1.history['val_loss'], label='SimpleRNN - Val Loss')

    plt.plot(history_2.history['loss'], label='LSTM - Train Loss')
    plt.plot(history_2.history['val_loss'], label='LSTM - Val Loss')

    plt.plot(history_3.history['loss'], label='GRU - Train Loss')
    plt.plot(history_3.history['val_loss'], label='GRU - Val Loss')

    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Loss Comparison')
    plt.legend()

    # Accuracy ë¹„êµ
    plt.subplot(1, 2, 2)
    plt.plot(history_1.history['accuracy'], label='SimpleRNN - Train Acc')
    plt.plot(history_1.history['val_accuracy'], label='SimpleRNN - Val Acc')

    plt.plot(history_2.history['accuracy'], label='LSTM - Train Acc')
    plt.plot(history_2.history['val_accuracy'], label='LSTM - Val Acc')

    plt.plot(history_3.history['accuracy'], label='GRU - Train Acc')
    plt.plot(history_3.history['val_accuracy'], label='GRU - Val Acc')

    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Accuracy Comparison')
    plt.legend()

    plt.show()

# ëª¨ë¸ ë¹„êµ ê·¸ë˜í”„ ì¶œë ¥
plot_comparison(history_1, history_2, history_3)